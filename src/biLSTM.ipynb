{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "sentences = [\"No me gusta el fútbol\", \"Nunca he estado tan feliz\"]\n",
    "tags = [[\"NEG\", \"O\", \"O\", \"O\", \"O\"], [\"NEG\", \"O\", \"O\", \"O\", \"O\"]]\n",
    "\n",
    "# Create vocabulary and tag mappings\n",
    "word_vocab = {\"<PAD>\": 0, \"No\": 1, \"me\": 2, \"gusta\": 3, \"el\": 4, \"fútbol\": 5, \"Nunca\": 6, \"he\": 7, \"estado\": 8, \"tan\": 9, \"feliz\": 10}\n",
    "tag_vocab = {\"O\": 0, \"NEG\": 1}\n",
    "word_vocab_size = len(word_vocab)\n",
    "tag_vocab_size = len(tag_vocab)\n",
    "\n",
    "# Preprocessing function to encode sentences and tags\n",
    "def encode_sentence(sentence, word_vocab):\n",
    "    return [word_vocab[word] for word in sentence.split()]\n",
    "\n",
    "def encode_tags(tags, tag_vocab):\n",
    "    return [tag_vocab[tag] for tag in tags]\n",
    "\n",
    "encoded_sentences = [encode_sentence(sentence, word_vocab) for sentence in sentences]\n",
    "encoded_tags = [encode_tags(tag, tag_vocab) for tag in tags]\n",
    "\n",
    "# Padding\n",
    "max_len = max(len(s) for s in encoded_sentences)\n",
    "padded_sentences = [s + [0] * (max_len - len(s)) for s in encoded_sentences]\n",
    "padded_tags = [t + [0] * (max_len - len(t)) for t in encoded_tags]\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(padded_sentences, dtype=torch.float32)\n",
    "y = torch.tensor(padded_tags, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class NegationDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = NegationDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class NegationDetectionModel(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(NegationDetectionModel, self).__init__()\n",
    "        \n",
    "        # BiLSTM Layer\n",
    "        self.bilstm = nn.LSTM(word_embedding_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Dense Layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim) # hidden_dim * 2 is done because is BIdirectional. Hence, we have the double dimensions\n",
    "        \n",
    "    def forward(self, word_embeds):\n",
    "        \n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.bilstm(word_embeds)\n",
    "        # Dense Layer\n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "word_embedding_dim = 5 # 300 word, 300 lemma, 11 pos\n",
    "hidden_dim = 5\n",
    "output_dim = word_embedding_dim\n",
    "num_layers=3\n",
    "# Instantiate the model\n",
    "model = NegationDetectionModel(word_embedding_dim, hidden_dim, num_layers, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, criterion, optimizer):\n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        for sequences, targets in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "            print(outputs, targets)\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1396, -0.3011,  0.3598, -0.3008, -0.0302],\n",
      "        [ 0.1276, -0.2851,  0.3378, -0.2934,  0.0175]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Epoch 1, Loss: 1.4844579696655273\n",
      "tensor([[ 0.1484, -0.2982,  0.3531, -0.3050, -0.0341],\n",
      "        [ 0.1394, -0.2819,  0.3448, -0.2955,  0.0080]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Epoch 2, Loss: 1.475182294845581\n",
      "tensor([[ 0.1504, -0.3019,  0.3550, -0.3056, -0.0381],\n",
      "        [ 0.1368, -0.2867,  0.3339, -0.2982,  0.0133]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Epoch 3, Loss: 1.4734771251678467\n",
      "tensor([[ 0.1590, -0.2990,  0.3486, -0.3095, -0.0421],\n",
      "        [ 0.1489, -0.2834,  0.3410, -0.3004,  0.0039]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Epoch 4, Loss: 1.4643020629882812\n",
      "tensor([[ 0.1611, -0.3025,  0.3503, -0.3103, -0.0459],\n",
      "        [ 0.1458, -0.2883,  0.3300, -0.3028,  0.0092]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Epoch 5, Loss: 1.462644100189209\n",
      "tensor([[ 0.1664, -0.3028,  0.3479, -0.3126, -0.0498],\n",
      "        [ 0.1503, -0.2891,  0.3280, -0.3051,  0.0071]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Epoch 6, Loss: 1.4572689533233643\n",
      "tensor([[ 0.1747, -0.3000,  0.3420, -0.3162, -0.0539],\n",
      "        [ 0.1630, -0.2854,  0.3354, -0.3077, -0.0021]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Epoch 7, Loss: 1.4482077360153198\n",
      "tensor([[ 0.1771, -0.3032,  0.3432, -0.3172, -0.0576],\n",
      "        [ 0.1593, -0.2905,  0.3242, -0.3096,  0.0029]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Epoch 8, Loss: 1.4465951919555664\n",
      "tensor([[ 0.1851, -0.3006,  0.3375, -0.3206, -0.0617],\n",
      "        [ 0.1723, -0.2866,  0.3317, -0.3124, -0.0061]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Epoch 9, Loss: 1.4376091957092285\n",
      "tensor([[ 0.1903, -0.3008,  0.3353, -0.3228, -0.0656],\n",
      "        [ 0.1769, -0.2872,  0.3298, -0.3147, -0.0080]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Epoch 10, Loss: 1.4323474168777466\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Nunca he estado tan feliz\n",
      "Predicted Tags: tensor([[ 0.1679, -0.2771,  0.3260, -0.3186, -0.0201]])\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "def test_model(model, test_loader, criterion):\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            #inputs = inputs.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"Average Test Loss: {avg_test_loss}\")\n",
    "# Test prediction\n",
    "test_sentence = \"Nunca he estado tan feliz\"\n",
    "predicted_tags = predict(model, test_sentence)\n",
    "print(\"Sentence:\", test_sentence)\n",
    "print(\"Predicted Tags:\", predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary to Google Drive\n",
    "#torch.save(model.state_dict(), '/content/drive/MyDrive/oscar/biLSTM.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
