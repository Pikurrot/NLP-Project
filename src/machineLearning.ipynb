{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn import preprocessing\n",
    "from sklearn_crfsuite import CRF\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pycrfsuite as crfs\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from time import time\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer funciron que lea las sentencias del documento de sentencias y las pase por la siguiente funcion iterativamente (solo las que tengan negación. Así que coger el fichero de negaciones y mirar si hay una en esa frase y entonce pasar)\n",
    "# Hacer una funcion que convierta cada sentencia en una lista de tokens con las features correspondientes\n",
    "# Pasar lo que devuelva la funcion a una lista mas grando, qie serà el training dataset\n",
    "# Hacer  una función para passar las sentencias al CRF y que se entrene (basado en el scope)\n",
    "# Hacer una función para que testear el funcionamiento del CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "11\n",
      "['B-NEG', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "43\n",
      "5\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-NEG', 'O']\n",
      "['O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-NEG', 'O', 'O']\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-NEG', 'O']\n",
      "['O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-NEG', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for xseq, yseq in zip(X_training, Y_training):\\n    print(xseq, yseq)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Spanish language model\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Function to read sentences from a file and store them in a dictionary\n",
    "def read_file(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            sentences.append(line.strip())\n",
    "    return sentences\n",
    "\n",
    "# Function to process text data\n",
    "def process_text(data, negations, output_file):\n",
    "    sentences=[]\n",
    "    # Iterate over each data item\n",
    "    #!for i in range(len(data)): LO HAGO SOLO CON EL PROMERO PARA PROBAR SI FUNCIONA Y PARA QUE VAYA RAPIDO\n",
    "    text = data[0][\"data\"][\"text\"]\n",
    "    sentences.extend(sent_tokenize(text))\n",
    "    return sentences\n",
    "\n",
    "def get_only_negated_sentences(sentences, negations):\n",
    "    negated_sentences=[]\n",
    "    for sentence in sentences:\n",
    "        for negation in negations:\n",
    "            pattern = r'\\b' + re.escape(negation) + r'\\b'\n",
    "            if re.search(pattern, sentence):\n",
    "                negated_sentences.append(sentence)\n",
    "                break\n",
    "    return negated_sentences\n",
    "\n",
    "def from_sentences_to_tokenfeatures(sentences):\n",
    "    feature_sentences=[]\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        feature_words=[]\n",
    "        # Process the sentence with SpaCy\n",
    "        doc = nlp(sentence)\n",
    "        \n",
    "        # Extract base forms, part-of-speech tags, chunk tags, and named entity tags\n",
    "        base_forms = [token.lemma_ for token in doc]\n",
    "        pos_tags = [token.pos_ for token in doc]\n",
    "        chunk_tags = [token.dep_ for token in doc]\n",
    "        \n",
    "        # Output the results in a tab-separated format\n",
    "        for token, lemma, pos, chunk in zip(doc, base_forms, pos_tags, chunk_tags):\n",
    "            #print(f\"{token.text}\\t{lemma}\\t{pos}\\t{chunk}\\t{ner}\")\n",
    "            feature_words.append([token.text, lemma, pos, chunk])\n",
    "        #print()  # Separate sentences with an empty line\n",
    "        feature_sentences.append(feature_words)\n",
    "\n",
    "    return feature_sentences\n",
    "\n",
    "def find_sublist(lst, sublist):\n",
    "    for i in range(len(lst) - len(sublist) + 1):\n",
    "        if lst[i:i+len(sublist)] == sublist:\n",
    "            return True, i\n",
    "    return False, -1\n",
    "\n",
    "def label_sublist(words_list, sublist, label, labeled_list=[]):\n",
    "    if(not labeled_list):\n",
    "        labeled_list = ['O'] * len(words_list)\n",
    "    found, start_index = find_sublist(words_list, sublist)\n",
    "    if not found:\n",
    "        return None\n",
    "\n",
    "    for i, word in enumerate(words_list):\n",
    "        if i == start_index:\n",
    "            labeled_list[i] = 'B-'+label\n",
    "        elif start_index < i < start_index + len(sublist):\n",
    "            labeled_list[i] = 'I-'+label\n",
    "\n",
    "    return labeled_list\n",
    "\n",
    "def from_sentences_to_BIO_tagging(sentences, words, label): #?Si añado labeled_words=[] parece que guarga lo del training data. Buscar una forma de añadir la lista. Puede que con un booleano extra para determinar si se borra la lista o no\n",
    "    #? ESTO HACERLO EN OTRA FUNCIÓN, PORQUE ES LA Y, Y LAS FEATURES SON LAS X, ADEMAS DE QUE LAS FEATUES SE REPETIRIAN CAVEZ QUE LLAMAMOS A LA FUNCIÓN PARA CALCULAR LOS BOI TAGS CORRESPONDIENTES\n",
    "    #? HACER UNA LISTA EN VEZ DE UN DICCIONARIO\n",
    "    #? BUSCAR COMO HACER QUE LA LISTA SE VAYA ACTUALIZANDO CADA VEZ QUE SE ENCUENTRA UN NUEVO BIO TAG, PARA QUE NO DESAPAREZCA EL ANTERIOR Y NO SEAN LISTAS DIFERENTES\n",
    "    #Extract BIO tagging for negations, uncertanties and its scopes\n",
    "    labeled_words=[]\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        sentence_list=[token.text for token in doc]\n",
    "        labeled_list=[]\n",
    "        for i, word in enumerate(words):\n",
    "            word_list=word.split()\n",
    "            \n",
    "            possible_labeled_list = label_sublist(sentence_list, word_list, label, labeled_list)\n",
    "            if possible_labeled_list:\n",
    "                labeled_list=possible_labeled_list\n",
    "                #print(str(sentence_list)+\" \"+str(len(sentence_list)))\n",
    "                #print(str(labeled_list)+\" \"+str(len(labeled_list)))\n",
    "        #print(str(labeled_list)+\" \"+str(len(labeled_list)))\n",
    "        labeled_words.append(labeled_list)\n",
    "    for i in labeled_words:\n",
    "        print(i)\n",
    "    return labeled_words\n",
    "\n",
    "# Define feature extraction function\n",
    "def token2features(tokens, i):\n",
    "    token = tokens[i]\n",
    "    features = {\n",
    "        'token': token[0],        # The token itself\n",
    "        'lemma': token[1],        # Lemma\n",
    "        'pos_tag': token[2],      # POS tagging\n",
    "        'chunk': token[3],        # Chunk\n",
    "        'bias': 1.0\n",
    "    }\n",
    "    if i > 0:\n",
    "        prev_token = tokens[i-1]\n",
    "        features.update({\n",
    "            'prev_token': prev_token[0],\n",
    "            'prev_pos_tag': prev_token[2],\n",
    "            'prev_chunk': prev_token[3]\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sequence\n",
    "    if i < len(tokens)-1:\n",
    "        next_token = tokens[i+1]\n",
    "        features.update({\n",
    "            'next_token': next_token[0],\n",
    "            'next_pos_tag': next_token[2],\n",
    "            'next_chunk': next_token[3]\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sequence\n",
    "    return features\n",
    "\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "negations_file = '../data/negation_cue.txt'\n",
    "output_file_training = \"../data/output_blind_negex_training.json\"\n",
    "output_file_testing = \"../data/output_blind_negex_testing.json\"\n",
    "\n",
    "# Load training data and process\n",
    "data_training = json.load(open(os.path.join(ROOT_DIR, \"data\", \"training_data.json\")))\n",
    "data_testing = json.load(open(os.path.join(ROOT_DIR, \"data\", \"test_data.json\")))\n",
    "negations = read_file(negations_file)\n",
    "\n",
    "sentences_train=process_text(data_training, negations, output_file_training)\n",
    "print(len(sentences_train))\n",
    "negated_sentences_train=get_only_negated_sentences(sentences_train, negations)\n",
    "print(len(negated_sentences_train))\n",
    "X_training=from_sentences_to_tokenfeatures(negated_sentences_train)\n",
    "Y_training=from_sentences_to_BIO_tagging(negated_sentences_train, negations, \"NEG\")\n",
    "\"\"\"y_train=from_sentences_to_BIO_tagging(negated_sentences, negations, \"NEG\", y_train)\n",
    "y_train=from_sentences_to_BIO_tagging(negated_sentences, negations, \"NEG\", y_train)\n",
    "y_train=from_sentences_to_BIO_tagging(negated_sentences, negations, \"NEG\", y_train)\"\"\"\n",
    "#TODO Me faltaria decir cual es la negación y el scope, con el BIO tagging. También pensar en como entrenar al CRF para que sepa que solo tiene que poner el scope y lo otro como O \n",
    "#TODO Continuación: podría decir que haga el BIO tagging de toda la frase, de tal modo que tambien diria el BIO tag de las negaciones. Claro, y el modelo entonces tendria que encontrar las negaciones y el scope, todo junto\n",
    "#TODO:\n",
    "# Pasar lo que devuelva la funcion a una lista mas grando, qie serà el training dataset\n",
    "# Hacer  una función para passar las sentencias al CRF y que se entrene (basado en el scope)\n",
    "# Hacer una función para que testear el funcionamiento del CRF\n",
    "\n",
    "# Hacer un txt para negations, otro para uncertanty, otro para scopes negation y otro para scopes uncertanty\n",
    "\n",
    "sentences_test=process_text(data_testing, negations, output_file_testing)\n",
    "print(len(sentences_test))\n",
    "negated_sentences_test=get_only_negated_sentences(sentences_test, negations)\n",
    "print(len(negated_sentences_test))\n",
    "X_testing=from_sentences_to_tokenfeatures(negated_sentences_test)\n",
    "Y_testing=from_sentences_to_BIO_tagging(negated_sentences_test, negations, \"NEG\")\n",
    "\n",
    "\"\"\"for xseq, yseq in zip(X_training, Y_training):\n",
    "    print(xseq, yseq)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ['B-NEG', 'O', 'O', 'O']\n",
      "52 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "12 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O']\n",
      "33 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O']\n",
      "26 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "28 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "47 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "19 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "27 ['O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "23 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O']\n",
      "32 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-NEG', 'O']\n",
      "['O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-NEG', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for sentence_tokens, sentence_labels in zip(X_training, Y_training):\n",
    "    X_sentence = [token2features(sentence_tokens, i) for i in range(len(sentence_tokens))]\n",
    "    y_sentence = sentence_labels\n",
    "    X_train.append(X_sentence)\n",
    "    y_train.append(y_sentence)\n",
    "\n",
    "\n",
    "# Train the CRF model\n",
    "trainer = crfs.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    print(len(xseq), yseq)\n",
    "    trainer.append(xseq, yseq)\n",
    "trainer.set_params({'c1': 1.0, 'c2': 1e-3, 'max_iterations': 50, 'feature.possible_transitions': True})\n",
    "trainer.train('../data/bio_crf.crfsuite')\n",
    "\n",
    "# Make predictions\n",
    "tagger = crfs.Tagger()\n",
    "tagger.open('../data/bio_crf.crfsuite')\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for sentence_tokens, sentence_labels in zip(X_testing, Y_testing):\n",
    "    X_sentence = [token2features(sentence_tokens, i) for i in range(len(sentence_tokens))]\n",
    "    y_sentence = sentence_labels\n",
    "    print(y_sentence)\n",
    "    X_test.append(X_sentence)\n",
    "    y_test.append(y_sentence)\n",
    "\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "3 ['O', 'B-NEG', 'O']\n",
      "18 ['O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O']\n",
      "39 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "4 ['O', 'B-NEG', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "for xseq, yseq in zip(X_test, y_test):\n",
    "    print(len(xseq), yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "3 ['O', 'O', 'O'] ['O', 'B-NEG', 'O']\n",
      "18 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O']\n",
      "39 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "4 ['O', 'O', 'O', 'O'] ['O', 'B-NEG', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "for sentence, y_p, y_t in zip(X_test, y_pred, y_test):\n",
    "    print(len(sentence), y_p, y_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
