{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *\n",
    "from crf import *\n",
    "import os\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "train_data_tokens = load_tokens(os.path.join(ROOT_DIR, \"data\", \"training_data_tokens.json\"))\n",
    "train_data = json.load(open(os.path.join(ROOT_DIR, \"data\", \"training_data.json\"), \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bio = create_bio_tags(train_data, train_data_tokens)\n",
    "with open(os.path.join(ROOT_DIR, \"data\", \"training_data_bio.json\"), \"w\") as f:\n",
    "\tjson.dump(train_data_bio, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRF(\n",
    "\tmodel_path=os.path.join(ROOT_DIR, \"models\", \"crf_0_0.crfsuite\"),\n",
    "\ttrainer_params={\n",
    "\t\t'c1': 1e-3,\n",
    "\t\t'c2': 1e-1,\n",
    "\t\t'max_iterations': 50,\n",
    "\t\t\"padding\": True,\n",
    "\t\t\"before_lim\": 6,\n",
    "\t\t\"after_lim\": 1,\n",
    "\t},\n",
    "\tverbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:31<00:00,  8.16it/s]\n"
     ]
    }
   ],
   "source": [
    "precompute_pos(\n",
    "\ttokens_path=os.path.join(ROOT_DIR, \"data\", \"training_data_tokens.json\"),\n",
    "\tpos_path=os.path.join(ROOT_DIR, \"data\", \"training_data_pos.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:00<00:00, 3208.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model trained\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "\ttrain_tokens_path=os.path.join(ROOT_DIR, \"data\", \"training_data_tokens.json\"),\n",
    "\ttrain_labels_path=os.path.join(ROOT_DIR, \"data\", \"training_data_bio.json\"),\n",
    "\ttrain_pos_path=os.path.join(ROOT_DIR, \"data\", \"training_data_pos.json\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:03<00:00, 79.58it/s]\n"
     ]
    }
   ],
   "source": [
    "model.process(\n",
    "\tdata_path=os.path.join(ROOT_DIR, \"data\", \"training_data.json\"),\n",
    "\ttokens_path=os.path.join(ROOT_DIR, \"data\", \"training_data_tokens.json\"),\n",
    "\tsave_path=os.path.join(ROOT_DIR, \"data\", \"training_data_predictions_crf.json\"),\n",
    "\tpos_path=os.path.join(ROOT_DIR, \"data\", \"training_data_pos.json\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision: 0.9797242276351673, Recall:0.9797242276351673, F1:0.9797242276351673\n"
     ]
    }
   ],
   "source": [
    "from eval import EvalOfficial\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, \"data\", 'training_data.json'), 'r', encoding='utf8') as _f:\n",
    "\ttrain_data = json.load(_f)\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, \"data\", 'training_data_predictions_crf.json'), 'r', encoding='utf8') as _f:\n",
    "\ttrain_data_predictions = json.load(_f)\n",
    "\n",
    "metric = EvalOfficial()\n",
    "p, r, f1 = metric.calc(train_data_predictions, train_data)\n",
    "print(\"Training\")\n",
    "print(f'Precision: {p}, Recall:{r}, F1:{f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:10<00:00,  6.29it/s]\n"
     ]
    }
   ],
   "source": [
    "model.process(\n",
    "\tdata_path=os.path.join(ROOT_DIR, \"data\", \"test_data.json\"),\n",
    "\ttokens_path=os.path.join(ROOT_DIR, \"data\", \"test_data_tokens.json\"),\n",
    "\tsave_path=os.path.join(ROOT_DIR, \"data\", \"test_data_predictions_crf.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Precision: 0.9595855897995074, Recall:0.9595855897995074, F1:0.9595855897995074\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(ROOT_DIR, \"data\", 'test_data.json'), 'r', encoding='utf8') as _f:\n",
    "\ttest_data = json.load(_f)\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, \"data\", 'test_data_predictions_crf.json'), 'r', encoding='utf8') as _f:\n",
    "\ttest_data_predictions = json.load(_f)\n",
    "\n",
    "metric = EvalOfficial()\n",
    "p, r, f1 = metric.calc(test_data_predictions, test_data)\n",
    "print(\"Testing\")\n",
    "print(f'Precision: {p}, Recall:{r}, F1:{f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating evaluation tokens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:21<00:00,  2.99it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 66.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training tokens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [01:12<00:00,  3.50it/s]\n",
      "100%|██████████| 254/254 [00:03<00:00, 72.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing training POS tags...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [01:00<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing evaluation POS tags...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:17<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLP models...\n"
     ]
    }
   ],
   "source": [
    "from eval import *\n",
    "from preprocessing import *\n",
    "import os\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "eval = EvalCRF(\n",
    "\tsave_dir=os.path.join(ROOT_DIR, \"temp\"),\n",
    "\tresults_dir=os.path.join(ROOT_DIR, \"data\"),\n",
    "\ttrain_data_path=os.path.join(ROOT_DIR, \"data\", 'training_data.json'),\n",
    "\teval_data_path=os.path.join(ROOT_DIR, \"data\", 'test_data.json'),\n",
    "\tload_existing_train_tokens=False,\n",
    "\tload_existing_eval_tokens=False,\n",
    "\tlemmatize=False,\n",
    "\tremove_punctuation=True,\n",
    "\treplace_numbers=None,\n",
    "\tverbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating CRF...\n",
      "Training CRF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:00<00:00, 2429.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model trained\n",
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 73.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.9622705694039893,\n",
       " 'recall': 0.9622705694039893,\n",
       " 'f1': 0.9622705694039893,\n",
       " 'time': 0.983}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_params={\n",
    "\t\t'c1': 1e-3,\n",
    "\t\t'c2': 1e-1,\n",
    "\t\t'max_iterations': 50,\n",
    "\t\t\"padding\": True,\n",
    "\t\t\"before_lim\": 6,\n",
    "\t\t\"after_lim\": 1,\n",
    "\t}\n",
    "eval.evaluate(**trainer_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
